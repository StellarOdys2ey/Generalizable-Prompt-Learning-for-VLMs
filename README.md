# Generalizable-Prompt-Learning-for-VLMs
A curated list of prompt learning methods for vision-language models which can be used for base-to-novel generalizaiton.

# <div style="font-size: 10px;">Table of Contents</div>

- [Papers](#papers)
    - [Published in 2025](#Published-in-2025)

Base-to-Novel Generalization. (ViT-B/16 CLIP)

## Published in 2025
- `TextRefiner` **TextRefiner: Internal Visual Feature as Efficient Refiner for Vision-Language Models Prompt Tuning**    AAAI 2025.     
[[Paper LinK](https://arxiv.org/abs/2412.08176)] [[Code Link](https://github.com/xjjxmu/TextRefiner)] 
- `ProText` **Learning to Prompt with Text Only Supervision for Vision-Language Models**    AAAI 2025.  
[[Paper Link](https://arxiv.org/abs/2401.02418)] [[Code Link](https://github.com/muzairkhattak/ProText)] ![](https://img.shields.io/badge/Text-green)
- `SPTR` **A Similarity Paradigm Through Textual Regularization Without Forgetting**    AAAI 2025.  
[[Paper Link](https://arxiv.org/abs/2502.14376)] [No code available] ![](https://img.shields.io/badge/Image--Text-blue)
- `FATE` **FATE: Feature-Adapted Parameter Tuning for Vision-Language Models**    AAAI 2025.
- [[Paper Link](https://arxiv.org/abs/2502.14376)] [No code available] ![](https://img.shields.io/badge/Text-green)
- `PTinCAS` **Prompt Tuning In a Compact Attribute Space**    AAAI 2025.
- [[Paper Link](https://ojs.aaai.org/index.php/AAAI/article/view/32365)] [No code available] ![](https://img.shields.io/badge/Text-green)
- `DsRA` **Exploring the Better Multimodal Synergy Strategy for Vision-Language Models**    AAAI 2025.  
[[Paper Link](https://ojs.aaai.org/index.php/AAAI/article/view/34372)] [No code available] ![](https://img.shields.io/badge/Image--Text-blue)
- `CLIP-AST` **Adaptive Parameter Selection for Tuning Vision-Language Models**    CVPR 2025.  
[[Paper Link](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Adaptive_Parameter_Selection_for_Tuning_Vision-Language_Models_CVPR_2025_paper.pdf)] [No code available] ![](https://img.shields.io/badge/Image--Text-blue)
- `MMRL` **MMRL: Multi-Modal Representation Learning for Vision-Language Models**    CVPR 2025.  
[[Paper Link](https://arxiv.org/abs/2503.08497)] [[Code Link](https://github.com/yunncheng/MMRL)] ![](https://img.shields.io/badge/Image--Text-blue)
- `DPC` **DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models**    CVPR 2025.   
[[Paper Link](https://arxiv.org/abs/2503.13443)] [[Code Link](https://github.com/JREion/DPC)] ![](https://img.shields.io/badge/Text-green)   
- `2SFS` **Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages**    CVPR 2025.   
[[Paper Link](https://arxiv.org/abs/2503.11609)] [[Code Link](https://github.com/FarinaMatteo/rethinking_fewshot_vlms)]   
- `SkipT` **Skip Tuning: Pre-trained Vision-Language Models are Effective and Efficient Adapters Themselves**    CVPR 2025.    
[[Paper Link](https://arxiv.org/abs/2412.01256)] [[Code Link](https://github.com/qunovo/NLPrompt)] ![](https://img.shields.io/badge/Text-green)   
- `TAC` **Task-Aware Clustering for Prompting Vision-Language Models**    CVPR 2025.   
[[Paper Link](https://openaccess.thecvf.com/content/CVPR2025/papers/Hao_Task-Aware_Clustering_for_Prompting_Vision-Language_Models_CVPR_2025_paper.pdf)] [[Code Link](https://github.com/FushengHao/TAC)] ![](https://img.shields.io/badge/Image--Text-blue)     
- `ATPrompt` **Advancing Textual Prompt Learning with Anchored Attributes**    ICCV 2025.   
[[Paper Link](https://arxiv.org/abs/2412.09442)] [[Code Link](https://github.com/zhengli97/ATPrompt)]![](https://img.shields.io/badge/Text-green)  
